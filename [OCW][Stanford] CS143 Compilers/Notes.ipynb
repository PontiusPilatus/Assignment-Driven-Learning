{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "collapsed": true,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# Notes for Stanford CS143 Compilers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Week 01: Introduction & the Cool Programming Language\n",
    "\n",
    "**How to Run the Program**\n",
    "\n",
    "- Compiler (Offline)\n",
    "  - $Program \\rightarrow Compiler \\rightarrow Execute$\n",
    "  - $Execute + Data \\rightarrow Output$\n",
    "- Interpreters (Online)\n",
    "  - $Program + Data \\rightarrow Interpreter \\rightarrow Output$\n",
    "\n",
    "**Compiler Concept**\n",
    "\n",
    "- (Syntactic) **Lexical Analysis**\n",
    "  - Concept: Divide program text into words or tokens.\n",
    "  - Input: text\n",
    "  - Output: words or tokens\n",
    "  - Sample Input: `if x == y then z = 1; else z = 2;`\n",
    "  - Sample Output: `#IF #ID(x) #EQAUL #ID(y) #THEN ...`\n",
    "- (Syntactic) **Parsing**\n",
    "  - Concept: Diagramming Sentences.\n",
    "  - Input: Tokens\n",
    "  - Output: Abstruct Semantic Tree\n",
    "  - Sample Input: #INT(5) #PLUS #INT(3) #MULTIPLY #INT(5)\n",
    "  - Sample Output: `(#PLUS (#INT(5))  (#MULTIPLY (#INT(3))  (#INT(5))))`\n",
    "- (Types scope) **Semantic Analysis**\n",
    "  - Concept: Catch inconsistencies.\n",
    "  - Sample Input: `{ int Jack=3; { int Jack=4; cout << Jack; } }`\n",
    "  - Question: What is the value?\n",
    "- **Optimization**\n",
    "  - Concept: Run faster/Use less memory/Use low power/network.\n",
    "- (Translation) **Code Generation**\n",
    "  - Concept: Produce assembly code.\n",
    "\n",
    "**Related Questions**\n",
    "\n",
    "- Why are there so many programming languages?\n",
    "  - Application domains have distinct/conflicting needs.\n",
    "- Why are there new programming languages?\n",
    "  - Programming training is the dominant cost for a programming language.\n",
    "  - Wild-used languages are **slow to change**.\n",
    "  - Easy to start a new language: when **productivity** > **training cost**\n",
    "  - Languages adopted to fill a void. (**Void** means new techniques.)\n",
    "- What is a good programming languages?\n",
    "  - There is **no** universally accepted metric for language design.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Week 02: Lexical Analysis & Finite Automata\n",
    "\n",
    "### 03 Lexical Analysis\n",
    "\n",
    "**Token Class (or Class)**\n",
    "\n",
    "Classify **program substrings** according to **role** (token class), **class** corresponding to **sets of strings**:\n",
    "\n",
    "- **Identifier** : string of letters or digits, starting with a letter\n",
    "- **Integer**: a non-empty string of digits\n",
    "- **Keyword**: else, if, begin, ...\n",
    "- **Whitespace**: a non-empty sequence of blanks, newlines, tabs\n",
    "\n",
    "**Goal of Lexical Analysis**\n",
    "\n",
    "- Definition\n",
    "\t- **lexeme**: A lexeme is a sequence of characters that matches the pattern for a token.\n",
    "\t- **token**: A token is a pair consisting of the token name and the value.\n",
    "- Concept\n",
    "\t- Parttition the input string into lexemes.\n",
    "\t- Identity the token of each lexeme.\n",
    "\t- Communicate tokens to the parser.\n",
    "- Input: Program Substrings\n",
    "- Output: Tokens\n",
    "- Sample Input: `string (foo=42)`\n",
    "- Sample Output: `<class, \"string\">, <'('>, <ID, \"foo\">, <Operator, \"=\">, <\"Int\", \"42>, <')'>`\n",
    "- Remark\n",
    "\t- **Left-to-Right** scan => **lookahead** required.\n",
    "\n",
    "#### Regular Languages\n",
    "\n",
    "Regular Expression:\n",
    "\n",
    "- Concept: Regular expressions (syntax) specify regular languages (set of strings).\n",
    "- 2 base cases\n",
    "  - Single Character: $'c' = \\{ \"c\" \\}$\n",
    "  - Empty String: $\\epsilon = \\{ \"\" \\}$\n",
    "- 3 compound expressions\n",
    "  - Union: $A + B = \\{a\\ |\\ a \\in A\\} \\cup \\{ b\\ |\\ b \\in B \\}$\n",
    "  - Concatenation: $AB = \\{ab\\ |\\ a \\in A \\wedge b \\in B \\}$\n",
    "  - Iteration: $\n",
    "A^* = \\bigcup_{i \\geq 0} A^i,\n",
    "\\begin{cases}\n",
    "A^i = A...A \\\\\n",
    "A^0 = \\epsilon\n",
    "\\end{cases}\n",
    "$\n",
    "\n",
    "**Def.** The **regular expression** over $\\Sigma$ are the smallest set of expressions including:\n",
    "$$\n",
    "\\begin{align}\n",
    "  R &= \\epsilon \\\\\n",
    "    &|\\ 'c', c\\in\\Sigma\\\\\n",
    "    &|\\ R+R\\\\\n",
    "    &|\\ RR\\\\\n",
    "    &|\\ R^*\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### Formal Languages\n",
    "\n",
    "**Def.** Let $\\Sigma$ be a set of characters (an alphabet).\n",
    "A **language** over $\\Sigma$ is a set of strings of characters drawn from $\\Sigma$.\n",
    "Meaning function $L$ maps **regular expressions** to **set of strings**.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "L(\\epsilon) &= \\{ \"\" \\} \\\\\n",
    "L('c') &= \\{ \"c\" \\} \\\\\n",
    "L(A+B) &= L(A) \\cup L(B) \\\\\n",
    "L(AB) &= \\{ab\\ |\\ a \\in L(A) \\wedge b \\in L(B)\\} \\\\\n",
    "L(A^*) &= \\bigcup_{i \\geq 0} L(A)^i \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "**Q**:Why use a meaning function?\n",
    "\n",
    "- Make clear what is syntax, what is semantics.\n",
    "- Allow us to consider notation as a seperate issue.\n",
    "- Because exp and meanings are not 1-1.\n",
    "  - Meaning is many to one.\n",
    "\n",
    "### 04 Lexical Specifications\n",
    "\n",
    "**Lexemes**\n",
    "\n",
    "- **Keyword**: \"if\" / \"else\" / \"then\" / ...\n",
    "  - $ 'if' + 'else' + 'then' + ...$\n",
    "- **Integer**: a non-empty string of digits\n",
    "  - $digits: '0' + '1' + '2' + ...$\n",
    "  - $(digit)(digit)^* = digit^+$\n",
    "- **Identifier**: strings of letters or digits, starting with a letter\n",
    "  - $letter = 'a' + 'b' + 'c' + ... = [a-zA-Z]$\n",
    "  - $(letter)(digit+letter)^*$\n",
    "- **Whitespace**: a non-empty sequence of blanks, newlines, and tabs\n",
    "  - $('\\;' + '\\setminus n' + '\\setminus t')^+$\n",
    "\n",
    "**More regular expressions**\n",
    "\n",
    "- At least one: $A^+ = AA^*$\n",
    "- Union: $A|B = A+B$\n",
    "- Option: $A? = A+\\epsilon$\n",
    "- Range: $'a'+'b'+\\dots+'z' = [a-z]$\n",
    "- Excluded Range: $\\widehat{[a-z]} = [\\wedge a-z]$\n",
    "\n",
    "**How do we check $program \\in L(R)$**?\n",
    "\n",
    "1. Write a regular expression for the lexemes of each token class\n",
    "\t- Number $= digit^+$\n",
    "\t- Keyword $= 'if'+'else'+\\dots$\n",
    "\t- Identifiew $= letter(letter+digit)^*$\n",
    "\t- OpenPar $='('$\n",
    "\t- ...\n",
    "2. Construct $R$ to match all lexemes.\n",
    "\t- $R = Number + Keyword + Number + ... = R_1 + R_2 + ...$\n",
    "3. Let input be $x_1...x_n$. Find the longest length $i$ such that $x_1...x_i \\in L(R)$.\n",
    "4. Remove $x_1...x_i$. Go to step 3.\n",
    "5. If $x_1...x_i \\in L(R_a) \\cap L(R_b)$, apply $R_{\\min(a,b)}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Week 03: Parsing & Top-Down Parsing\n",
    "\n",
    "### Parsing\n",
    "\n",
    "What is **parsing**?\n",
    "\n",
    "- **Goal**: To derive the parse tree of program in grammar.\n",
    "- **Input**: Sequence of tokens from lexer\n",
    "- **Output**: Parse tree of the program.\n",
    "- Sample\n",
    "\t- Input: `IF ID = ID THEN INT ELSE INT FI`\n",
    "\t- Output: `(IF-THEN-ELSE (= ID ID) (INT) (INT))`\n",
    "\n",
    "#### Context-Free Grammars\n",
    "\n",
    "- **Goal**: To separate the token string into many pieces such that every piece is acceptable in grammar.\n",
    "\t- **Context-Free** means every two pieces are not related.\n",
    "\t- Parser must distinguish between **valid** and **invalid** strings of tokens under CFGs.\n",
    "- What we need\n",
    "\t- A language for **describing valid strings of tokens**.\n",
    "\t- A method for distinguishing valid under the language.\n",
    "- Note. Programming languages always have recursive structures.\n",
    "\t- An **EXPR** is ... (recursive)\n",
    "\t\t- if EXPR then EXPR else EXPR fi\n",
    "\t- Context-free grammars are a natural notation for this recursive structure.\n",
    "- Def. A CFG consists of\n",
    "\t- A set of terminals $T$\n",
    "\t- A set of non-terminals $N$\n",
    "\t- A start symbol $S \\in N$\n",
    "\t- A set of productions $X \\rightarrow Y_1, \\dots, Y_N, X\\in N, Y_i \\in N\\cup T\\cup \\{ \\epsilon\\}$\n",
    "\t\t- Ex: $S\\rightarrow (S)$\n",
    "\t\t- How the production works?\n",
    "\t\t\t- Begin with a string with only the start symbol $S$\n",
    "\t\t\t- Replace any non-terminal $X$ in the string by the right-hand side of some production $X \\rightarrow Y_1\\dots Y_n$\n",
    "\t\t\t- Repeat (2) until there are no non-terminals\n",
    "\t\t\t\t- $X_1\\dots X_i X X_{i+1}\\dots X_N \\rightarrow X_1\\dots X_i Y_1\\dots Y_k X_{i+1}\\dots X_N,\\ if\\ X\\rightarrow Y_1\\dots Y_k$\n",
    "\t\t\t\t- $S \\rightarrow \\dots \\rightarrow \\alpha_0 \\rightarrow \\alpha_1 \\rightarrow \\dots \\rightarrow \\alpha_n, if\\ \\alpha_0 \\rightarrow^* \\alpha_n (\\geq 0\\ steps)$\n",
    "- Def. (Language) Use grammar to define language.\n",
    "\t- Let $G$ be a context-free grammar with start symbol $S$. Then the language $L(G)$ of $G$ is $$\\{ a_1\\dots a_n\\ |\\ \\forall i\\ a_i\\in T\\wedge S\\rightarrow^* a_1\\dots a_n \\}$$\n",
    "\t- Terminals ought to be tokens of the language.\n",
    "\t- Membership in a language is **yes** or **no**; also need parse tree of the input.\n",
    "\t- Must handle errors gracefully.\n",
    "\t- Need an implementation of CFG’s (e.g., bison)\n",
    "\t- Form of the grammar is important.\n",
    "\t\t- Many grammars generate the same language.\n",
    "\t\t- Tools are sensitive to the grammar.\n",
    "\n",
    "#### Derivations\n",
    "\n",
    "- **Goal**: To define the process from the start symbol to some valid token string.\n",
    "\t- Def. A **derivation** is a sequence of productions applying on a start symbol $S$ to a sequences without any non-terminal.\n",
    "\t- We can draw a derivation as a tree\n",
    "\t\t- Start symbol is the tree’s root\n",
    "\t\t- For a production $X \\rightarrow Y_1\\dots Y_n$ add children $Y_1\\dots Y_n$ to node $X$.\n",
    "\t- Sample\n",
    "\t\t- $G$: $E\\rightarrow E+E\\ |\\ E*E\\ |\\ (E)\\ |\\ id$\n",
    "\t\t- Target: $id*id+id$\n",
    "\t\t- Derivation: $E \\rightarrow E+E \\rightarrow E*E+E \\rightarrow id*E+E \\rightarrow id*id+E \\rightarrow id*id+id$\n",
    "\t\t- Draw derivation as a parse tree: $(((id) * (id)) + (id))$\n",
    "\t\t- The example is a **left-most** derivation. At each step, replace the left-most non-terminal.\n",
    "\t\t- Note that right-most and left-most derivations have the same parse tree.\n",
    "- A parse tree has\n",
    "\t- Terminals at the leaves\n",
    "\t- Non-terminals at the interior nodes\n",
    "- An **in-order** traversal of the leaves is the original input.\n",
    "- We are not just interested in whether $s \\in L(G)$.\n",
    "\t- We need a parse tree for $s$.\n",
    "- The relation between derivation and parse tree are many to one.\n",
    "\t- `((id)+(id))`\n",
    "\n",
    "#### Ambiguity\n",
    "\n",
    "- **Goal**: Parse tree is used to describe how we explain the program string. If there are two parse trees for the program, it has two meanings.\n",
    "- Sample\n",
    "\t- $G$: $E\\rightarrow E+E\\ |\\ E*E\\ |\\ (E)\\ |\\ id$\n",
    "\t- Target: $id*id+id$\n",
    "\t- This string has two parse trees\n",
    "\t\t- Derivation: $E \\rightarrow E+E \\rightarrow E*E+E \\rightarrow id*E+E \\rightarrow id*id+E \\rightarrow id*id+id$\n",
    "\t\t- Derivation: $E \\rightarrow E*E \\rightarrow E*E+E \\rightarrow id*E+E \\rightarrow id*id+E \\rightarrow id*id+id$\n",
    "- Def. A grammar is **ambiguous**, if it has more than one parse tree for some string $s\\in L(G)$.\n",
    "\t- A lot of ways to solve it.\n",
    "\t- Most direct method is to rewrite grammar unambiguously.\n",
    "\t- Like, enforces precedence of * over +.\n",
    "\t- Impossible to convert **automatically** an ambiguous grammar to an unambiguous one.\n",
    "\t- Most tools allow precedence and associativity declarations to disambiguate grammars.\n",
    "\n",
    "#### Error\n",
    "\n",
    "Error Handling\n",
    "\n",
    "- **Goal**\n",
    "\t- To detect non-valid programs\n",
    "\t- To translate the valid ones\n",
    "\t- Sample\n",
    "\t\t- (Lexer) Errors in Lexical: `...$...`\n",
    "\t\t- (Parser) Errors in Syntax: `...x *%...`\n",
    "\t\t- (Type checker) Errors in Semantic: `...int x; y=x(3);...`\n",
    "\t\t- (Tester/User) Errors in Correctness: program\n",
    "\t\n",
    "Error Handler\n",
    "\n",
    "- **Goal**\n",
    "\t- Report errors accurately and clearly\n",
    "\t- Recover from an error quickly\n",
    "\t- Not slow down compilation of valid code\n",
    "- How to do\n",
    "\t- Panic mode\n",
    "\t\t- Discard tokens until one with a clear role is found. And continue.\n",
    "\t\t- Bison: use the special terminal error to describe how much input to skip.\n",
    "\t- Error productions\n",
    "\t\t- Specify known common mistakes in the grammar, like `5x` and `5*x`...\n",
    "\t\t- Complicates the grammar.\n",
    "\t- Automatic local or global correction\n",
    "\t\t- Try token insertions and deletions. (Edit distance)\n",
    "\t\t- Exhaustive search.\n",
    "\n",
    "#### Abstract Syntax Trees (AST)\n",
    "\n",
    "- **Goal**: Represent the parse tree and ignore the details.\n",
    "- **Form**: `(op (op1) (op2),...,(opn))`\n",
    "\t- `(IF-THEN-ELSE (< (x) (5)) (= (y) (4)) (= (y) (3)))`\n",
    "\n",
    "#### Top-Down Parsing\n",
    "\n",
    "- Recursive Descent Parsing\n",
    "\t- The parse tree is constructed\n",
    "\t\t- From the top\n",
    "\t\t- From left to right\n",
    "\t- Use DFS to match the token string\n",
    "\t\t- Let the global `next` point to the next input token: `TOKEN *next = array;`\n",
    "\t\t- Try to match a given token terminal: `bool term(TOKEN tok) { return *next++ == tok; }`\n",
    "\t\t- Try to mathc the $n^{th}$ production of S: `bool Sn() { ... }`\n",
    "\t\t- For production $E\\rightarrow T$: `bool E1() { return T(); }`\n",
    "\t\t- For production $E\\rightarrow T+E$: `bool E2() { return T() && term(PLUS) && E(); }`\n",
    "\t\t- For all productions of E (with backtracking) ```\n",
    "\t\t\tbool E() { TOKEN *save = next; return (next = save, E1()) || (next = save, E2()); }```\n",
    "- Error in RD\n",
    "\t- Scenario: $S \\rightarrow Sa$\n",
    "\t\t- It goes into an infinite loop.\n",
    "\t- RD does not work in **left-recursive grammar**.\n",
    "\t\t- Def. A **left-recursive grammar** has a non-terminal $S$, $S\\rightarrow^+ S\\alpha$, for some $\\alpha$\n",
    "\t\t- Rewrite all production rules to use right-recursion.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Week 04: Bottom-Up Parsing I & II\n",
    "\n",
    "### Predictive Parser: LL(1)\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "- Concept\n",
    "  - Look at the next ? tokens.\n",
    "  - No backtracking.\n",
    "  - Accept LL(K) grammars. (Left-to-Right, Leftmost derivation, k tokens)\n",
    "  - At each step, only 1 choice.\n",
    "  - To avoid ambiguousness, need to **left-factor** the grammar.\n",
    "- Parsing Table: Leftmode Non-terminal x Next Input Token\n",
    "  - Use stack to record frontier of parse tree\n",
    "- Differnce from Recursive Descent\n",
    "  - For the leftmost non-terminal **S**\n",
    "  - Look at the next input token **a**\n",
    "  - Choose the production shown at **[S,a]**\n",
    "- Reject on reaching error state\n",
    "- Accept on (end of input && empty stack)\n",
    "\n",
    "#### First Set\n",
    "\n",
    "**Def**. (First Set)\n",
    "\n",
    "$$First(X)=\\{t\\ |\\ X\\rightarrow^*t\\alpha\\}\\cup\\{\\epsilon\\ |\\ X\\rightarrow^*\\epsilon\\}$$\n",
    "\n",
    "**Algo**\n",
    "\n",
    "1. $First(X)=\\{t\\}, if\\ t\\ is\\ a\\ terminal.$\n",
    "2. $\\epsilon \\in First(X), if\n",
    "\\begin{cases}\n",
    "X\\rightarrow \\epsilon \\\\\n",
    "X\\rightarrow A_1A_2\\dots A_n, and\\ \\epsilon \\in First(A_i), \\forall 1\\leq i\\leq n\n",
    "\\end{cases}$\n",
    "3. $First(\\alpha) \\subseteq First(X), if\\ X\\rightarrow A_1A_2\\dots A_n\\alpha, and\\ \\epsilon \\in First(A_i), \\forall 1\\leq i\\leq n$\n",
    "\n",
    "#### Follow Set\n",
    "\n",
    "**Def**. (Follow Set)\n",
    "\n",
    "$$Follow(X)=\\{t\\ |\\ S\\rightarrow^*\\beta Xt\\delta\\}$$\n",
    "\n",
    "**Algo**\n",
    "\n",
    "1. $\\$ \\in Follow(S)$\n",
    "2. $First(\\beta)-\\{\\epsilon\\}\\subseteq Follow(X), for\\ each\\ A\\rightarrow\\alpha X\\beta$\n",
    "3. $Follow(A) \\subseteq Follow(X), for\\ each A\\rightarrow\\alpha X\\beta\\ where\\ \\epsilon\\in First(\\beta)$\n",
    "\n",
    "#### How to Construct LL(1) Parsing Tables\n",
    "\n",
    "- Construct a parsing table **T** for CFG **G**\n",
    "- For each production $A \\rightarrow \\alpha$ in G do:\n",
    "  - For each terminal $t \\in First(\\alpha)$, $T[A, t] = \\alpha$\n",
    "  - If $\\epsilon \\in First(\\alpha),$, for each $t \\in Follow(A)$, $T[A,t] = \\alpha$\n",
    "  - If $\\epsilon \\in First(\\alpha)$ and $ \\$ \\in Follow(A)$, $T[A,\\$] = \\alpha$\n",
    "- If any entry is multiply defined, then G is not LL(1).\n",
    "\n",
    "### Bottom-Up Parsing \n",
    "\n",
    "#### Shift-Reduce Parsing\n",
    "\n",
    "- Concept\n",
    "  - Don't need **left-factored** grammars.\n",
    "  - Reduce the string to the start symbol. (Inverting production)\n",
    "  - A bottom-up parser traces a rightmost derivation in reverse. $$\n",
    "\\begin{align}\n",
    "& int*int + int \\\\\n",
    "& \\textbf{int*T} + int \\\\\n",
    "& \\textbf{T} + int \\\\\n",
    "& T + \\textbf{T} \\\\\n",
    "& T + \\textbf{E} \\\\\n",
    "& \\textbf{E}\n",
    "\\end{align}\n",
    "$$\n",
    "  - Thm. \n",
    "    - In some step, let string as $\\alpha \\beta \\gamma$.\n",
    "    - Assume the next reduction is by $X \\rightarrow \\beta$. ($\\alpha \\beta \\gamma \\rightarrow \\alpha X \\gamma$)\n",
    "    - Then $\\gamma$ is a string which contains only terminals.\n",
    "  - Split the string as $L_{Str} | R_{Str}$.\n",
    "  - $L_{Str}$ contains non-terminals and terminals. $R_{Str}$ contains only terminals.\n",
    "- Actions\n",
    "  - **Shift**: Move | one place to the right. Shift a terminal to the left string.\n",
    "  - **Reduce**: Apply an inverse production at the right of $L_{str}$. $$\n",
    "for\\ A \\rightarrow xy, Cbxy|ijk \\Rightarrow CbA|ijk $$\n",
    "- Implementation\n",
    "  - Use a stack to maintain $L_{Str}$.\n",
    "- If is's legal to shift or reduce, there is a **shift-reduce** conflict.\n",
    "- If is's legal to reduce by 2 productions, there is a **reduce-reduce** conflict.\n",
    "\n",
    "#### Term. (Handle)\n",
    "\n",
    "- Scenario\n",
    "  - Grammar: $E \\rightarrow T+E|E$ & $T \\rightarrow int*T|int|(E)$\n",
    "  - At the steop $int|*int+int$\n",
    "  - If we reduce by ($T \\rightarrow int$) given ($T | *int+int$), it will fail.\n",
    "- Target: Want to reduce only if the result can still be reduced to the start symbol.\n",
    "- A **handle** is the rhs of the production that you can trace back in the parse tree.\n",
    "- Assume a rightmost derivation $$S \\rightarrow^* \\alpha X \\omega \\rightarrow \\alpha\\beta\\omega$$\n",
    "  - Then $\\beta$ is a **handle** of $\\alpha \\beta \\omega$.\n",
    "- Thm.\n",
    "  - In shift-reduce parsing, handles appear only **at the top of the stack**, never inside.\n",
    "- Bottom-up parsing algorithms are based on recognizing handles\n",
    "\n",
    "#### Recognizing Handles\n",
    "\n",
    "- There are no known efficient algo to recognize handles. But there are good heuristics.\n",
    "- **Viable Prefix**: $\\alpha_1\\dots\\alpha_n$ is a **viable prefix** if there is an $\\omega$ s.t. $\\alpha_1\\dots\\alpha_n\\ |\\ \\omega$ is a state of a shift-reduce parser.\n",
    "  - A viable prefix is always the prefix of some handle.\n",
    "  - If we can maintain the stack's contents are viable prefixes all the time, no parsing error will occur.\n",
    "- Thm. For any grammar, the set of viable prefixes is a **regular language**.\n",
    "- **Item**: An **item** is a production with '.' somewhere on the rhs.\n",
    "  - Example. The items for $T \\rightarrow (E)$ are\n",
    "    - $T \\rightarrow .(E)$\n",
    "    - $T \\rightarrow (.E)$\n",
    "    - $T \\rightarrow (E.)$\n",
    "    - $T \\rightarrow (E).$\n",
    "  - Example. The items for ($X \\rightarrow \\epsilon$) are $X \\rightarrow .$\n",
    "  - Items are often called **LR(0) items**.\n",
    "- Target: To describe the states of the production rule.\n",
    "  - If we need to use some production rule $R$ to reduce, the top of the stack will be the left string split by '.' on some item of $R$.\n",
    "  - Example.\n",
    "    - Input: $(int)$\n",
    "    - Grammar: $E \\rightarrow T+E\\ |\\ T$ and $T \\rightarrow int*T\\ |\\ int\\ |\\ (E)$\n",
    "    - $(E$ is the left string split by '.' on the item $T \\rightarrow (E.)$\n",
    "\n",
    "\n",
    "#### Recognizing Viable Prefixes\n",
    "\n",
    "Algo.\n",
    "1. Add a dummy production $S' \\rightarrow S$ to $G$\n",
    "2. The NFA states are the items of $G$\n",
    "3. For item $E \\rightarrow \\alpha .X\\beta$ add transition: $(E \\rightarrow \\alpha . X \\beta) \\rightarrow^X (E \\rightarrow \\alpha X.\\beta)$\n",
    "4. For item $E \\rightarrow \\alpha .X\\beta$ and production $X \\rightarrow \\gamma$, add transition: $(E \\rightarrow \\alpha . X \\beta) \\rightarrow^\\epsilon (X \\rightarrow .\\gamma)$\n",
    "5. Every state is an accepting state\n",
    "6. Start state is $S' \\rightarrow .S$\n",
    "\n",
    "#### Valid Items\n",
    "\n",
    "- Concept\n",
    "  - Item $X \\rightarrow \\beta .\\gamma$ is **valid** for a viable prefix $\\alpha\\beta$ if $$S' \\rightarrow^* \\alpha X\\omega \\rightarrow \\alpha\\beta\\gamma\\omega$$ by a right-most derivation.\n",
    "  - An item $I$ is valid for a viable prefix $\\alpha(\\alpha_1\\dots\\alpha_n)$ if the DFA accepts $\\alpha_1\\dots\\alpha_n$ and terminates on some state $s$ containing $I$.\n",
    "  - The items in $s$ describe what the top of the item stack might be after reading input $\\alpha_1\\dots\\alpha_n$.\n",
    "  \n",
    "\n",
    "#### LR(0) Parsing\n",
    "\n",
    "- Assume\n",
    "  - Stack contains $\\alpha_1\\dots\\alpha_k$\n",
    "  - Next input is $t$\n",
    "  - DFA on input $\\alpha_1\\dots\\alpha_k$ teminates in state $s$\n",
    "- Action\n",
    "  - Reduce by $X \\rightarrow \\beta$ if $s$ contains item $X \\rightarrow \\beta.$\n",
    "  - Shift if $s$ contains item $X \\rightarrow \\beta .t\\omega$\n",
    "- Conflict\n",
    "  - Reduce/Reduce if any state contains two reduce rule.\n",
    "  - Shift/Reduce if any state has a reduce item and a shift item\n",
    "  \n",
    "#### Simple LR Paring (SLR(1) Parsing)\n",
    "\n",
    "- Assume\n",
    "  - Stack contains $\\alpha_1\\dots\\alpha_k$\n",
    "  - Next input is $t$\n",
    "  - DFA on input $\\alpha_1\\dots\\alpha_k$ teminates in state $s$\n",
    "- Action\n",
    "  - Reduce by $X \\rightarrow \\beta$ if $s$ contains item $X \\rightarrow \\beta.$ and $t \\in Follow(X)$\n",
    "  - Shift if $s$ contains item $X \\rightarrow \\beta .t\\omega$\n",
    "- If there are conflicts under these rules, the grammar is not SLR.\n",
    "- We can use **precedence declarations** to resolve conflicts. \\* is grater than \\+.\n",
    "- LR(1)\n",
    "  - More powerful than SLR(1)\n",
    "  - Build by (item x lookahead).\n",
    "  - SLR(1) only checks whether lookahead is in follow set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Week 05: Semantic Analysis and Type Checking\n",
    "\n",
    "### Semantic Analysis\n",
    "\n",
    "#### Introduction\n",
    "\n",
    "- Error detection\n",
    "  - Lexical Analysis: Detects inputs with **illegal tokens**\n",
    "  - Parsing: Detects inputs with **ill-formed parse trees**\n",
    "  - Semantic Analysis: **Catches all remaining errors**\n",
    "    - Example. (Cool)\n",
    "      1. All idenfifiers are declared\n",
    "      2. Types\n",
    "      3. Inheritance relationships\n",
    "      4. Classes defined only once\n",
    "      5. ...\n",
    "\n",
    "#### Scope\n",
    "\n",
    "- Matching identifier declarations with uses.\n",
    "- The scope of an identifier is the portion of a program in which that identifier is accessible\n",
    "- The same identifier may refer to different things in different parts of the program.\n",
    "- Most languages have static scope.\n",
    "- A few languages are dynamically scoped.\n",
    "  - A dynamically-scoped variable refers to the closest enclosing binding in the execution of the program.\n",
    "- Cool identifier bindings are introduced by\n",
    "  - Class declarations (introduce class names)\n",
    "  - Method definitions (introduce method names)\n",
    "  - Let expressions (introduce object id’s)\n",
    "  - Formal parameters (introduce object id’s)\n",
    "  - Attribute definitions (introduce object id’s)\n",
    "  - Case expressions (introduce object id’s)\n",
    "- Not all identifiers follow the most-closely nested rule.\n",
    "  - Example. (class definitions in Cool)\n",
    "    - Are globally visible throughout the program.\n",
    "- Attribute names are global within the class in which they are defined.\n",
    "- Methods may also be redefined (overridden).\n",
    "\n",
    "#### Symbol Tables\n",
    "\n",
    "- Much of semantic analysis can be expressed as a recursive descent of an AST\n",
    "  - Before: Push the identifiers into symbol tables (stack).\n",
    "  - Recure: Process the children of AST.\n",
    "  - After: Pop the identifier up from symbol tables (stack).\n",
    "  - Use stack because there are identifier definitions with the same name.\n",
    "- Symbol Table Operations\n",
    "  - `add_symbol(x)`\n",
    "  - `find_symbol(x)`\n",
    "  - `remove_symbol(x)`\n",
    "  - `enter_scope()`\n",
    "  - `exit_scopt()`\n",
    "  - `check_scope(x)`\n",
    "- Because class can be used before being defined, how to manage class?\n",
    "  - Solution. 2 Pass\n",
    "    - Pass 1: Gather all class names\n",
    "    - Pass 2: Do the checking\n",
    "  - Actually, semantic analysis requires **multiple** passes.\n",
    "\n",
    "### Type Checking\n",
    "\n",
    "#### Types\n",
    "\n",
    "- Meaning: The notion varies in different languages.\n",
    "- Consensus\n",
    "  - A set of values\n",
    "  - A set of operations on those values\n",
    "- In assembly language level, there are no types. For `add $r1, $r2, $r3`, just add it bitwisely.\n",
    "- Same type, but not make sense.\n",
    "  - Function Pointer (int) + Number (int)\n",
    "- Type system in a language specifies which operations are valid for which types.\n",
    "- The goal of **type checking** is to ensure that operations are used only with the correct type.\n",
    "- Three kinds of languages\n",
    "  - Statically typed: All or almost all checking of types is done as part of compilation.\n",
    "  - Dynamically typed: Almost all checking of types is done as part of program execution.\n",
    "  - Untyped: No type checking.\n",
    "- Competing view\n",
    "  - Static Typing\n",
    "    - Static checking catches many programming errors at compile time.\n",
    "    - Avoids overhead of runtime type checks.\n",
    "  - Dynamic Typing\n",
    "    - Static type systems are restrictive.\n",
    "    - Rapid prototyping difficult within a static type system.\n",
    "- A lot of code is written in statically typed languages with an “escape” mechanism. Unsafe casts in C, Java.\n",
    "- People retrofit static typing to dynamically typed languages for optimization, debugging.\n",
    "- The compiler infers types for every expression.\n",
    "- Term.\n",
    "  - **Type Checking** is the process of verifying fully typed programs.\n",
    "  - **Type Inference** is the process of filling in missing type information.\n",
    "  \n",
    "#### Type Checking\n",
    "\n",
    "- We have seen two examples of formal notation specifying parts of a compiler.\n",
    "  - Lexical Analysis: Regular expressions\n",
    "  - Parsing: Context-free grammars\n",
    "  - Type checking: Logical rules of inference\n",
    "- Inference Rules\n",
    "  - Form: If **hypothesis** is true, then **Conclusion** is true.\n",
    "  - Type checking: If $E_1$ and $E_2$ have certain types, then $E_3$ has a certain type.\n",
    "  - Notation\n",
    "    - And: $\\wedge$\n",
    "    - If-Then: $\\Rightarrow$\n",
    "    - $x$ has type $T$: $x:T$\n",
    "    - It is provable that...: $\\vdash$\n",
    "- Example.\n",
    "  - If $e_1$ has type **Int** and $e_2$ has type **Int**, then $e_1+e_2$ has type **Int**.\n",
    "  - ($e_1$ has type **Int** $\\wedge$ $e_2$ has type **Int**) $\\Rightarrow$ $e_1+e_2$ has type **Int**.\n",
    "  - $(e_1:Int \\wedge e_2:Int) \\Rightarrow e_1+e_2:Int$\n",
    "  - $Hypothesis_1 \\wedge \\dots \\wedge Hypothesis_n \\Rightarrow Conclusion$\n",
    "- Tradition inference rules are writen by $$\\frac{\\vdash Hypothesis \\dots \\vdash Hypothesis}{\\vdash Conclusion}$$\n",
    "- Cool type rules: $\\vdash e:T$\n",
    "- Example.\n",
    "  - [Int] $$\\frac{i:Constant}{\\vdash i:Int}$$\n",
    "  - [Add] $$\\frac{\\vdash e_1:Int \\vdash e_2:Int}{\\vdash e_1+e_2:Int}$$\n",
    "  - [1+2] $$\\frac{\\frac{1:Constant}{\\vdash 1:Int}\\frac{2:Constant}{\\vdash 2:Int}}{\\vdash 1+2:Int}$$\n",
    "- A type system is **sound** if\n",
    "  - When $\\vdash e:T$, then $e$ evaluates to a value of type $T$.\n",
    "  - We only want sound rules and the more prescise one. $(\\vdash i:Int) > (\\vdash i:Object)$\n",
    "- Types are computed in a bottom-up pass over the AST.\n",
    "\n",
    "#### Type Environment\n",
    "\n",
    "- Other rules\n",
    "  - [False] $\\frac{}{\\vdash false: Bool}$\n",
    "  - [String] $\\frac{s:StringConstant}{\\vdash s:String}$\n",
    "  - (Ignore **SELF_TYPE**) **new T** produce an object of type **T** \n",
    "    - [New] $\\frac{}{\\vdash new T:T}$\n",
    "  - [Not] $\\frac{\\vdash e: Bool}{\\vdash !e:Bool}$\n",
    "  - [Loop] $\\frac{\\frac{\\vdash e_1:Bool}{\\vdash e_2:T}}{\\vdash while\\ e_1 loop\\ e_2 pool:Object}$\n",
    "  - [Var] $\\frac{x\\ is\\ a\\ variable}{\\vdash x: ?}$\n",
    "    - We need the type rule.\n",
    "- **Type Environment**\n",
    "  - A **type environment** gives types for **free** variables.\n",
    "    - A type environment is a function from **Object Identifiers** to **Types**\n",
    "    - A variable is **free** in a expression if it is not defined within the expression.\n",
    "  - Let $O$ be a function from **Object Identifiers** to **Types**.\n",
    "  - Writen by: $O\\vdash e:T$\n",
    "  - Add to every rules.\n",
    "    - [Int] $$\\frac{i:Constant}{O\\vdash i:Int}$$\n",
    "    - [Add] $$\\frac{O\\vdash e_1:Int\\ O\\vdash e_2:Int}{O\\vdash e_1+e_2:Int}$$\n",
    "    - [Var] $$\\frac{O(x)=T}{O\\vdash x:T}$$\n",
    "    - [Let-No-Init] $$\\frac{O[T_0/x]\\vdash e_1:T_1}{O\\vdash let\\ x:T_0\\ in\\ e_1:T_1}$$\n",
    "    - Remark\n",
    "      - $O[T/x](x) = T$\n",
    "      - $O[T/x](y) = O(y)$\n",
    "  - Remark\n",
    "    - The type environment gives types to the free identifiers in the current scope.\n",
    "    - The type environment is passed down the AST from the root towards the leaves.\n",
    "    - Types are computed up the AST from the leaves towards the root.\n",
    "\n",
    "#### Subtyping\n",
    "\n",
    "- Consider **let** rule\n",
    "  - [Let-Init] $$\\frac{O\\vdash e_0:T_0\\\\O[T_0/x]\\vdash e_1:T_1}{O\\vdash let\\ x:T_0\\leftarrow e_0\\ in\\ e_1:T_1}$$\n",
    "- Define a relation $\\leq$ on classes\n",
    "  - $X\\leq X$\n",
    "  - $X\\leq Y$ if $X$ inherits from $Y$\n",
    "  - $X\\leq Z$ if $X\\leq Y$ and $Y\\leq Z$\n",
    "    - [Let-Init] $$\\frac{O\\vdash e_0:T_0\\\\O[T_0/x]\\vdash e_1:T_1\\\\T_0\\leq T}{O\\vdash let\\ x:T\\leftarrow e_0\\ in\\ e_1:T_1}$$\n",
    "    - [Assign] $$\\frac{O(x)=T_0\\\\O\\vdash e_1:T_1\\\\T_1\\leq T_0}{O\\vdash x\\leftarrow e_1:T_1}$$\n",
    "- Let $O_c(x)=T$ for all attributes $x:T$ in class $C$\n",
    "  - [Attr-Init] $$\\frac{O_c(x)=T_0\\\\O_c\\vdash e_1:T_1\\\\T_1\\leq T_0}{O_c\\vdash x:T_0\\leftarrow e_1}$$\n",
    "- How to decide the return type of if statement?\n",
    "  - if $e_0$ then $e_1$ else $e_2$ fi\n",
    "  - The best way we can do is the smallest **supertype** larger than the type of $e_1$ or $e_2$.\n",
    "  - $lub(X,Y)$, the least upper bound of $X$ and $Y$, is $Z$ if \n",
    "    - $X\\leq Z \\wedge Y \\leq Z$, $Z$ is an upper bound.\n",
    "    - $X\\leq Z' \\wedge Y \\leq Z' \\Rightarrow Z\\leq Z'$, $Z$ is an upper bound.\n",
    "  - In COOL, the least upper bound of two types is their **least common ancestor** in the inheritance tree.\n",
    "  - [If-Then-Else] $$\\frac{O\\vdash e_0:Bool\\\\O\\vdash e_1:T_1\\\\O\\vdash e_2:T_2}{O\\vdash if\\ e_0\\ then\\ e_1\\ else\\ e_2\\ fi:lub(T_1,T_2)}$$\n",
    "  - [Case] $$\\frac{O\\vdash e_0:T_0\\\\O[T_1/x_1]\\vdash e_1:T_{1`}\\\\\\dots\\\\ O[T_n/x_n]\\vdash e_n:T_{n'}}{O\\vdash case\\ e_0\\ of\\ x_1:T_1\\rightarrow e_1;\\dots ;x_n:T_n\\rightarrow e_n;esac:lub(T_1,T_2)}$$\n",
    "  \n",
    "#### Typing Methods\n",
    "\n",
    "- In COOL, method and object identifiers live in different namespaces.\n",
    "  - A method **foo** and an object **foo** can coexist in the same scope.\n",
    "  - We use a separate mapping $M$ For method signatures $$M(C,f)=(T_1,\\dots,T_n,T_{n+1})$$ means in class $C$ there is a method $f$ $$f(x_1:T_1,\\dots,x_n:T_n):T_{n+1}$$\n",
    "  - [Dispatch] $$\\frac{O,M\\vdash e_0:T_0\\\\ O,M\\vdash e_1:T_1\\\\ \\dots\\\\ O,M\\vdash e_n:T_n\\\\ M(T_0, f)=(T_{1'},\\dots,T_{n'},T_{n+1})\\\\ T_i\\leq T_{i'},\\ for\\ 1\\leq i\\leq n}{O,M\\vdash e_0.f(e_1,\\dots,e_n):T_{n+1}}$$\n",
    "  - [Static Dispatch] $$\\frac{O,M\\vdash e_0:T_0\\\\ O,M\\vdash e_1:T_1\\\\ \\dots\\\\ O,M\\vdash e_n:T_n\\\\ T_0\\leq T\\\\ M(T, f)=(T_{1'},\\dots,T_{n'},T_{n+1})\\\\ T_i\\leq T_{i'},\\ for\\ 1\\leq i\\leq n}{O,M\\vdash e_0@T.f(e_1,\\dots,e_n):T_{n+1}}$$\n",
    "- The **method environment** must be added to all rules.\n",
    "- In most cases, $M$ is passed down but not actually used. (Only the dispatch rules use $M$)\n",
    "  - [Add] $$\\frac{O,M\\vdash e_1:Int\\ O,M\\vdash e_2:Int}{O,M\\vdash e_1+e_2:Int}$$\n",
    "- To derive the type of **SELF_TYPE**, we need to know the class in which an expression appears.\n",
    "- The full type environment for COOL\n",
    "  - A mapping $O$ giving types to object id's\n",
    "  - A mapping $M$ giving types to methods\n",
    "  - The current class $C$\n",
    "  - [Add] $$\\frac{O,M,C\\vdash e_1:Int\\ O,M,C\\vdash e_2:Int}{O,M,C\\vdash e_1+e_2:Int}$$\n",
    "- General Themes\n",
    "  - Type rules are defined on the structure of expressions.\n",
    "  - Types of variables are modeled by an environment.\n",
    "  \n",
    "#### Implementation of Typing Checking\n",
    "\n",
    "- COOL type checking can be implemented in a single traversal over the AST.\n",
    "- Type environment is passed down the tree from parent to child.\n",
    "- Types are passed up the tree from child to parent.\n",
    "  - [Add] $$\\frac{O,M,C\\vdash e_1:Int\\ O,M,C\\vdash e_2:Int}{O,M,C\\vdash e_1+e_2:Int}$$\n",
    "  - ```c\n",
    "    TypeCheck(Environment, e1+e2) = {\n",
    "      T1 = TypeCheck(Environment, e1);\n",
    "      T2 = TypeCheck(Environment, e2);\n",
    "      Check T1 == T2 == Int;\n",
    "      return Int;\n",
    "    }\n",
    "    ```\n",
    "  - [Let-Init] $$\\frac{O\\vdash e_0:T_0\\\\O[T_0/x]\\vdash e_1:T_1\\\\T_0\\leq T}{O\\vdash let\\ x:T\\leftarrow e_0\\ in\\ e_1:T_1}$$\n",
    "  - ```c\n",
    "    TypeCheck(Environment, let x:T <- e0 in e1) = {\n",
    "      T0 = TypeCheck(Environment, e0);\n",
    "      T1 = TypeCheck(Environment.add(x:T), e1);\n",
    "      Check subtype(T0,T);\n",
    "      return T1;\n",
    "    }\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "deletable": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Week 06: Cool Type Checking & Runtime Organization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "## Week 07: Code Generation & Operational Semantics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "\n",
    "## Week 08: Local Optimization & Global Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "\n",
    "## Week 09: Register Allocation & Garbage Collection\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {}
   },
   "source": [
    "## Week 10: Java"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
